{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a5bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala duckdb no notebook\n",
    "%pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ddf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "motherduck_token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6InpoYW5nNDg5eXVhbkBnbWFpbC5jb20iLCJzZXNzaW9uIjoiemhhbmc0ODl5dWFuLmdtYWlsLmNvbSIsInBhdCI6IktkU2Z0TzVsZmdoVmdBTWJNbDZKNHM0QXZjeXNISWFCUGxmODhFMHJseGciLCJ1c2VySWQiOiJmZWIwZWM2Ni1jMGEwLTQ2NTgtYTgxMS01OTVlMTYyOTM0YWQiLCJpc3MiOiJtZF9wYXQiLCJyZWFkT25seSI6ZmFsc2UsInRva2VuVHlwZSI6InJlYWRfd3JpdGUiLCJpYXQiOjE3NDQwNTk5NTl9.n_CImtcF2YtJFQ8x2GUWSACVSj8-eXS4NtHwKaVXT4E\"\n",
    "con = duckdb.connect(f\"md:?token={motherduck_token}\")\n",
    "\n",
    "df = con.execute(\"select * from sample_data.hn.hacker_news limit 10\").df()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ========================\n",
    "# Initialize Spark\n",
    "# ========================\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ========================\n",
    "# Example DataFrame\n",
    "# ========================\n",
    "#data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "#columns = [\"name\", \"age\"]\n",
    "#df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df_spark = spark.createDataFrame(df)\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Define catalog, schema, and table\n",
    "# ========================\n",
    "catalog = \"main_catalog\"\n",
    "schema = \"analytics_schema\"\n",
    "table_name = \"people\"\n",
    "full_table_name = f\"{catalog}.{schema}.{table_name}\"\n",
    "\n",
    "# ========================\n",
    "# Create catalog and schema if not exists\n",
    "# ========================\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "\n",
    "# ========================\n",
    "# Write DataFrame to Unity Catalog\n",
    "# ========================\n",
    "df_spark.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(full_table_name)\n",
    "\n",
    "#.option(\"overwriteSchema\", \"true\") \\\n",
    "\n",
    "print(f\"Table {full_table_name} saved successfully!\")\n",
    "\n",
    "# ========================\n",
    "# Verify table\n",
    "# ========================\n",
    "#spark.sql(f\"SELECT * FROM {full_table_name}\").show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
